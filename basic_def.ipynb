{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'Avanish', 'Gupta', '.', 'I', 'am', 'going', 'to', 'learn', 'it', 'from', 'begning', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text=\"I am Avanish Gupta. I am going to learn it from begning.\"\n",
    "tokens=nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag=nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('Avanish', 'JJ'),\n",
       " ('Gupta', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('going', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('learn', 'VB'),\n",
       " ('it', 'PRP'),\n",
       " ('from', 'IN'),\n",
       " ('begning', 'VBG'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRP\n",
      "  am/VBP\n",
      "  Avanish/JJ\n",
      "  Gupta/NNP\n",
      "  ./.\n",
      "  I/PRP\n",
      "  am/VBP\n",
      "  going/VBG\n",
      "  to/TO\n",
      "  learn/VB\n",
      "  it/PRP\n",
      "  from/IN\n",
      "  begning/VBG\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "grammer= \"NP: {<NN><DT>?<JJ>*<NN>}\"\n",
    "cp=nltk.RegexpParser(grammer)\n",
    "result=cp.parse(tag)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afer pos_tagging \n",
      "('super', 'JJ')\n",
      "('fast', 'NN')\n",
      "('processor', 'NN')\n",
      "('and', 'CC')\n",
      "('really', 'RB')\n",
      "('nice', 'JJ')\n",
      "('graphics', 'NNS')\n",
      "('card..and', 'VBP')\n",
      "('plenty', 'NN')\n",
      "('of', 'IN')\n",
      "('storage', 'NN')\n",
      "('with', 'IN')\n",
      "('250', 'CD')\n",
      "('gb', 'NN')\n",
      "('(', '(')\n",
      "('though', 'IN')\n",
      "('I', 'PRP')\n",
      "('will', 'MD')\n",
      "('upgrade', 'VB')\n",
      "('this', 'DT')\n",
      "('and', 'CC')\n",
      "('the', 'DT')\n",
      "('ram..', 'NN')\n",
      "(')', ')')\n",
      "('This', 'DT')\n",
      "('computer', 'NN')\n",
      "('is', 'VBZ')\n",
      "('really', 'RB')\n",
      "('fast', 'JJ')\n",
      "('and', 'CC')\n",
      "('I', 'PRP')\n",
      "(\"'m\", 'VBP')\n",
      "('shocked', 'JJ')\n",
      "('as', 'IN')\n",
      "('to', 'TO')\n",
      "('how', 'WRB')\n",
      "('easy', 'JJ')\n",
      "('it', 'PRP')\n",
      "('is', 'VBZ')\n",
      "('to', 'TO')\n",
      "('get', 'VB')\n",
      "('used', 'VBN')\n",
      "('to..', 'NN')\n"
     ]
    }
   ],
   "source": [
    "text=\"super fast processor and really nice graphics card..and plenty of storage with 250 gb(though I will upgrade this and the ram..)This computer is really fast and I'm shocked as to how easy it is to get used to..\"\n",
    "token=nltk.word_tokenize(text)\n",
    "pos=nltk.pos_tag(token)\n",
    "print(\"Afer pos_tagging \")\n",
    "for i in range(len(pos)):\n",
    "    print(pos[i])\n",
    "#grammer=\"NP: {<NN><NN>}\"\n",
    "#cp=nltk.RegexpParser(grammer)\n",
    "#result=cp.parse(tag)\n",
    "#print(\"After parsing :\",result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_check(text):\n",
    "    l=len(text)\n",
    "    lst=[]\n",
    "    for i in range(l):\n",
    "        print(text[i],type(text[i]),text[i][1])\n",
    "    for i in range(l-1):\n",
    "        if text[i][1]=='NN' and text[i+1][1]=='NN':\n",
    "            pp=[]\n",
    "            pp.append(text[i][0])\n",
    "            pp.append(text[i+1][0])\n",
    "            lst.append(pp)\n",
    "    return lst\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('super', 'JJ') <class 'tuple'> JJ\n",
      "('fast', 'NN') <class 'tuple'> NN\n",
      "('processor', 'NN') <class 'tuple'> NN\n",
      "('and', 'CC') <class 'tuple'> CC\n",
      "('really', 'RB') <class 'tuple'> RB\n",
      "('nice', 'JJ') <class 'tuple'> JJ\n",
      "('graphics', 'NNS') <class 'tuple'> NNS\n",
      "('card..and', 'VBP') <class 'tuple'> VBP\n",
      "('plenty', 'NN') <class 'tuple'> NN\n",
      "('of', 'IN') <class 'tuple'> IN\n",
      "('storage', 'NN') <class 'tuple'> NN\n",
      "('with', 'IN') <class 'tuple'> IN\n",
      "('250', 'CD') <class 'tuple'> CD\n",
      "('gb', 'NN') <class 'tuple'> NN\n",
      "('(', '(') <class 'tuple'> (\n",
      "('though', 'IN') <class 'tuple'> IN\n",
      "('I', 'PRP') <class 'tuple'> PRP\n",
      "('will', 'MD') <class 'tuple'> MD\n",
      "('upgrade', 'VB') <class 'tuple'> VB\n",
      "('this', 'DT') <class 'tuple'> DT\n",
      "('and', 'CC') <class 'tuple'> CC\n",
      "('the', 'DT') <class 'tuple'> DT\n",
      "('ram..', 'NN') <class 'tuple'> NN\n",
      "(')', ')') <class 'tuple'> )\n",
      "('This', 'DT') <class 'tuple'> DT\n",
      "('computer', 'NN') <class 'tuple'> NN\n",
      "('is', 'VBZ') <class 'tuple'> VBZ\n",
      "('really', 'RB') <class 'tuple'> RB\n",
      "('fast', 'JJ') <class 'tuple'> JJ\n",
      "('and', 'CC') <class 'tuple'> CC\n",
      "('I', 'PRP') <class 'tuple'> PRP\n",
      "(\"'m\", 'VBP') <class 'tuple'> VBP\n",
      "('shocked', 'JJ') <class 'tuple'> JJ\n",
      "('as', 'IN') <class 'tuple'> IN\n",
      "('to', 'TO') <class 'tuple'> TO\n",
      "('how', 'WRB') <class 'tuple'> WRB\n",
      "('easy', 'JJ') <class 'tuple'> JJ\n",
      "('it', 'PRP') <class 'tuple'> PRP\n",
      "('is', 'VBZ') <class 'tuple'> VBZ\n",
      "('to', 'TO') <class 'tuple'> TO\n",
      "('get', 'VB') <class 'tuple'> VB\n",
      "('used', 'VBN') <class 'tuple'> VBN\n",
      "('to..', 'NN') <class 'tuple'> NN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['fast', 'processor']]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_check(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_all(text):\n",
    "    l=len(text)\n",
    "    two_noun=[]\n",
    "    for i in range(l-1):\n",
    "        if text[i][1]=='NN' and text[i+1][1]=='NN':\n",
    "            pp=[]\n",
    "            pp.append(text[i][0])\n",
    "            pp.append(text[i+1][0])\n",
    "            two_noun.append(pp)\n",
    "    verb=[]\n",
    "    adjective=[]\n",
    "    adverb=[]\n",
    "    noun=[]\n",
    "    for i in range(l):\n",
    "        if text[i][1]=='VB':\n",
    "            verb.append(text[i][0])\n",
    "        elif text[i][1]=='JJ':\n",
    "            adjective.append(text[i][0])\n",
    "        elif text[i][1]=='RB':\n",
    "            adverb.append(text[i][0])\n",
    "        elif text[i][1]=='NN':\n",
    "            noun.append(text[i][0])\n",
    "            \n",
    "    return two_noun,verb,adjective,adverb,noun\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fast', 'processor']]\n",
      "['upgrade', 'get']\n",
      "['super', 'nice', 'fast', 'shocked', 'easy']\n",
      "['really', 'really']\n",
      "['fast', 'processor', 'plenty', 'storage', 'gb', 'ram..', 'computer', 'to..']\n"
     ]
    }
   ],
   "source": [
    "words=check_all(pos)\n",
    "for w in words:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avani\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.sentiment as se\n",
    "import xml.etree.ElementTree as ET\n",
    "f=open(r'ABSA16_Laptops_Train_SB1_v2.xml','r')\n",
    "data=f.read()\n",
    "dt=ET.parse('ABSA16_Laptops_Train_SB1_v2.xml')\n",
    "root=dt.getroot()\n",
    "ip=[]\n",
    "op=[]\n",
    "for child in root:\n",
    "    for child1 in child:\n",
    "        for child2 in child1:\n",
    "            i=0\n",
    "            t=[]\n",
    "            for child3 in child2:\n",
    "                if i==0:\n",
    "                    #print(child3.text)\n",
    "                    ip.append(child3.text)\n",
    "                else:\n",
    "                    for child4 in child3:\n",
    "                        #print(child4.attrib)\n",
    "                        t.append(child4.attrib)\n",
    "                i=i+1\n",
    "            op.append(tuple(t))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Being a PC user my whole life....', 'This computer is absolutely AMAZING!!!', '10 plus hours of battery...', 'super fast processor and really nice graphics card..', 'and plenty of storage with 250 gb(though I will upgrade this and the ram..)']\n"
     ]
    }
   ],
   "source": [
    "print(ip[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-50f587a9b0dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msent_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"hello World\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " from nltk.tokenize import word_tokenize \n",
    "tokens=word_tokenize(str(ip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43402,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.shape(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "token_ip=[[w for w in s if w not in stopwords.words('english')] for s in word_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(token_ip)):\n",
    "    print(len(token_ip[i]),token_ip[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-5b121aceb62d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_ip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_ip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mtoken_ip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_ip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# removing spaces\n",
    "count=0\n",
    "for i in range(len(token_ip)):\n",
    "    if len(token_ip[i]) is 0:\n",
    "        token_ip.remove(token_ip[i])\n",
    "        count=count+1\n",
    "print(\"Total number of spaces is \",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(type(token_ip[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "word_tokens=tokenizer.tokenize(str(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34379,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "numpy.shape(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "token_ip=[[w for w in s if w not in stopwords.words('english')] for s in word_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34379,)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.shape(token_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "lst=[]\n",
    "print(type(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "if len(lst) is 0:\n",
    "    print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
